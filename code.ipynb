{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Made a prediction of survivied in the Titanic\n",
    "\n",
    "\n",
    "To introduce ourselves into the concepts previusly expoused, we use a Kaagle competition about the Titanic.\n",
    "\n",
    "This competition is one of the most popular to introduce into the Machin Learning world. The idea is knowing a part of what happend to the Titanic passengers, predict what would happens to the rest.\n",
    "\n",
    "First of all we need to install python in our system. Also we need to install some libraries to manage data.\n",
    "\n",
    "Specific we will go to use that libraries:\n",
    "\n",
    "    pandas: It’s  allow us to manage data in diferent formats (in our case .csv)\n",
    "    numpy: to work with arrays, vectors and other data set\n",
    "    scikit-learn: A specific library for Machine learning bring us different structures to predict.\n",
    "    \n",
    "We use too other library to Ignore warning, because Jupyter launch a lot of warnings about the diferents versions of Python.\n",
    "\n",
    "The easiest way to start to work in Python is to install Anaconda, a package manager which include a lot of popular libraries include of course this ones.\n",
    "\n",
    "To create our prediction we need to go to the website of the competition and download two documents:\n",
    "\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "The first of a list of the passanger of the Titanic and include a field that says us if they survived or not. The second one is the same document but without this field and help us to prove our models.\n",
    "\n",
    "To edit with Python we can use our preferred text editor as SublimeText or Atom, but Anaconda also install us an editor called jupyter so usefull for Python and is the one we go to use.\n",
    "\n",
    "Now we can take a look to our data using a few  usefull method of the panda library: head(), that show us the five first rows of our file...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train dataset\n",
    "train_url = \"dataset/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "# Take a look to the content\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... describe() thats as his name says describe the content of the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and shape what say us the dimensions of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method gives us two the name of the columns. As we can see we have a column called ‘Survived’ to access to it pandas bring us the standard bracket notation to select a single column and with the method  .value_counts() have an idea of what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"Survived\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we works with data is a good idea to have the results in percentage rather than absolute. Of course panda bring us a simple parameter, which we can pass to the value_counts method, to achive it; normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of Survivor column\n",
    "print(train[\"Survived\"].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can deduce if you board the Titanic the usual is you died.\n",
    "\n",
    "Then if we predict about an amount of people board the Titanic, we can say all of they died. This calculation give us a 62% hit rate \n",
    "\n",
    "But maybe we can analyze a little more our data and see for example if the sex is important for the survival if you board the Titanic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of male survival\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True))\n",
    "\n",
    "# Percentage of female survival\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if you are a woman board the Titanic you have more possibilities to survive (74%) than if you are a men who only survive the 19%.\n",
    "\n",
    "Then, if we have to do a prediction we can say if you are a woman the usual is you survive and not if you are a man."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do our first prediction take in account only the sex of the passengers. To do this we go to take the sheet called test, that is the one provided to us to prove our algorithm. We make a copy of it, and create our prediction.\n",
    "\n",
    "If we want to upload our prediction to Kaggle we need to follow the requirements of the competition and summision.\n",
    "\n",
    "We can read it the competition here: https://www.kaggle.com/c/titanic#evaluation\n",
    "\n",
    "There we can read:\n",
    "\n",
    "    Submission File Format\n",
    "    You should submit a csv file with exactly 418 entries plus a header row. \n",
    "    Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n",
    "    The file should have exactly 2 columns:\n",
    "    PassengerId (sorted in any order)\n",
    "    Survived (contains your binary predictions: 1 for survived, 0 for deceased) \n",
    "\n",
    "Then we go to build it and download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Load the test dataset\n",
    "test_url = \"dataset/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "# Duplicate our dataset\n",
    "myTestTest = test.copy()\n",
    "\n",
    "# Initialize a Survived column to 0\n",
    "myTestTest['Survived'] = 0\n",
    "\n",
    "# Set Survived to 1 if Sex equals \"female\"\n",
    "myTestTest[\"Survived\"][myTestTest[\"Sex\"] == 'female'] = 1\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myTestTest[\"PassengerId\"]).astype(int)\n",
    "Survived = np.array(myTestTest[\"Survived\"]).astype(int)\n",
    "testSolution = pd.DataFrame(Survived, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name firstSolution.csv\n",
    "testSolution.columns = ['Survived']\n",
    "testSolution.to_csv(\"testSolution.csv\", index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we upload our solution to Kaggle we can see we achive a score of 76%, what is not bad, (we have to keep in mind that is so complicated to predict the events, and a score over 80% it's a very good result), but really this is so simple way because we do it manually as we put by ourselves the value of the Survived column.\n",
    "\n",
    "To improve this we go to construct a decision tree which helps us with our prediction.\n",
    "\n",
    "* #### Decision Tree: \n",
    "A decision tree is a logical structure in wich you enter some information y following some rules give a result for it.\n",
    "\n",
    "\n",
    "First we go to duplicate the data file to don't modify the original. After this we have to normalize our data. We go to give a numeric data wich is easy to read to the data we go to use. We go to assign 1 to female and 0 to male in the column “Sex”. After this, we go to asing 0 to class S, 1 to class C and 2 to class Q in the Embarked colum. And to be sure all the passangers are in one class we go first to fill the possible gaps with the most commun class wich is S. To know this we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train[\"Embarked\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalice too the column \"Age\", we will go to assing this ones who don’t have one, the age average of all of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicate our dataset\n",
    "myTrain = train.copy()\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "myTrain[\"Sex\"][myTrain[\"Sex\"] == \"male\"] = 0\n",
    "myTrain[\"Sex\"][myTrain[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Fill the gaps in the Embarked variable\n",
    "myTrain[\"Embarked\"] = myTrain[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"S\"] = 0\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"C\"] = 1\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "# Fill the gaps in the Age variable\n",
    "myTrain[\"Age\"] = myTrain[\"Age\"].fillna(myTrain[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have harmonize our data we can start creating our first decision tree. We go to use another popular library to do it, sklearn. With this library we can create a tree. First of all we create a tree, and after this we say what we want achive (target), and which elements we want include in our decision tree to achieve it (features).\n",
    "\n",
    "Now we can see wan importance give the programme to each of our features with the attribute .feature_importances_, and see what score we have with this tree with the method score().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "# Create the target and features \n",
    "target = myTrain[\"Survived\"].values\n",
    "features_one = myTrain[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we achive with this method a good prediction of more than 97%, what is a very good prediction.\n",
    "The decision tree create by our program is very complicate, but if we want to study or just to know it, we can create a version of it generation a .dot file (the file used by sklearn library) adding this to the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"decisionTree.dot\", 'w') as archivo_dot:\n",
    "  \t    tree.export_graphviz(my_tree_one, out_file = archivo_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to convert the dot file to a jpg we can use a program from the graphviz site: http://www.graphviz.org/Download_windows.php\n",
    "To avoid all this steps and allow you to see the result I do it before and the decision tree is that:\n",
    "\n",
    "![caption](img/firstDecisionTree.png)\n",
    "\n",
    "As you can see is very complicated but the important thing is his efficiency.\n",
    "\n",
    "Now we go to use this decision tree to make our second prediction.\n",
    "\n",
    "To make work our perdiction algorithm, we need have complete information for all subjets, and if we analiced the test dataset, we can see the fare information in the position 152 is empty, then we give to it the median of the rest of subjects. \n",
    "\n",
    "After this (an doing in the test database the same operations we do in the train dataset) we can put to work our algorithm  and if we want upload the result to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicate our dataset\n",
    "myFirstTest = test.copy()\n",
    "\n",
    "# Impute the missing value with the median\n",
    "myFirstTest.Fare[152] = myFirstTest[\"Fare\"].median()\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "myFirstTest[\"Sex\"][myFirstTest[\"Sex\"] == \"male\"] = 0\n",
    "myFirstTest[\"Sex\"][myFirstTest[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Fill the gaps in the Embarked variable\n",
    "myFirstTest[\"Embarked\"] = myFirstTest[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Fill the gaps in the Age variable\n",
    "myFirstTest[\"Age\"] = myFirstTest[\"Age\"].fillna(myFirstTest[\"Age\"].median())\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"S\"] = 0\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"C\"] = 1\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "myFirstTest_features = myFirstTest[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Make a prediction using the test set\n",
    "myFirstprediction = my_tree_one.predict(myFirstTest_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myFirstTest[\"PassengerId\"]).astype(int)\n",
    "firstSolution = pd.DataFrame(myFirstprediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name firstSolution.csv\n",
    "firstSolution.columns = ['Survived']\n",
    "firstSolution.to_csv(\"firstSolution.csv\", index_label = [\"PassengerId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we upload our solution to kaggle we will obtain a disappointing score of around 70%  Why? This is a good moment to explain other interesting concept of the Machine Learning: the  Overfitting.\n",
    "    \n",
    "* #### Overfitting:\n",
    "When we training our model it’s possible we do it so well but only to the data we have for example. The idea for a model it’s to generalize after with other data, not only with ones used to train. But in this case, we construct a decision tree so complicated that fit perfectly to the data we use to train. Actually, we have for our model more than 97%, but when we use it for other data we achieve only a 70%<br>\n",
    "This is the problem of the overfitting. To avoid it we limit the deeper of the decision tree to have more capacity to generalice. To do it we use two arguments whe we buid our decision tree. max_depth what determines when the splitting up of the decision tree stops and min_samples_split what stop when don’t have enough elements.<br>\n",
    "One of the decisions have to take the data science is swing well this two variables to achieve the best value, but in any case we can modify it to see how the score varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit your first decision tree: my_tree_one\n",
    "max_depth = 4\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_one, target)\n",
    "\n",
    "print(my_tree_two.score(features_one, target))\n",
    "\n",
    "# Make a prediction using the test set\n",
    "mySecondPrediction = my_tree_two.predict(myFirstTest_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myFirstTest[\"PassengerId\"]).astype(int)\n",
    "secondSolution = pd.DataFrame(mySecondPrediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name secondSolution.csv\n",
    "secondSolution.columns = ['Survived']\n",
    "secondSolution.to_csv(\"secondSolution.csv\", index_label = [\"PassengerId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also we can create the image to view the new decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"secondSolution.dot\", 'w') as archivo_dot:\n",
    "  \t    tree.export_graphviz(my_tree_two, out_file = archivo_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see more simple\n",
    "\n",
    "![caption](img/secondDecisionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, this only is to introduce the overfitting concept. Now, if we really want to made a good prediction we must improve our algorithm including all the possible important variable and also, we can create new ones if we thik is good for the job. In this case, I'll go to create one called family_size, thinking that maybe the largest families lost more time looking for each other and have lower probability of surviving. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "myTrain_two = myTrain.copy()\n",
    "myTrain_two[\"family_size\"] = myTrain_two[\"SibSp\"] +  myTrain_two[\"Parch\"] + 1\n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = myTrain_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"SibSp\", \"Parch\", \"family_size\"]].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_three = my_tree_three.fit(features_three, target)\n",
    "print(my_tree_three.score(features_three, target))\n",
    "\n",
    "# Duplicate our test dataset\n",
    "mySecondTest = myFirstTest.copy()\n",
    "\n",
    "# Create mySecondTest with the newly defined feature\n",
    "mySecondTest[\"family_size\"] = mySecondTest[\"SibSp\"] +  mySecondTest[\"Parch\"] + 1\n",
    "\n",
    "# Create the new test feature \n",
    "mySecondTest_features = mySecondTest[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"SibSp\", \"Parch\", \"family_size\"]].values\n",
    "\n",
    "# Make a prediction using the test set\n",
    "mySecondPrediction = my_tree_three.predict(mySecondTest_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(mySecondTest[\"PassengerId\"]).astype(int)\n",
    "thirdSolution = pd.DataFrame(mySecondPrediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name secondSolution.csv\n",
    "thirdSolution.columns = ['Survived']\n",
    "thirdSolution.to_csv(\"thirdSolution.csv\", index_label = [\"PassengerId\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree improve our score in Kaggle, but we can still get better score in our prediction using other interesting concept of Machine Learning: The Random Forest.\n",
    "\n",
    "* #### Random Forest:\n",
    "A Random Forest is basically a Forest of Decision Trees. We can manage a lot of decisions tree and for each items we evaluated all the results for each decision tree, and assingn the final result to the most commun issue.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features_forest = myTrain_two[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 4, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "test_features = mySecondTest[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "\n",
    "# Make a prediction using the test set\n",
    "myLastPrediction = my_forest.predict(test_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(mySecondTest[\"PassengerId\"]).astype(int)\n",
    "LastSolution = pd.DataFrame(myLastPrediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name secondSolution.csv\n",
    "LastSolution.columns = ['Survived']\n",
    "LastSolution.to_csv(\"LastSolution.csv\", index_label = [\"PassengerId\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Whith this code we obtain a score over 80%. It's a good prediction, but also we can improve it using other formulas and finding other variables in our prediction, for example, it's important the situation of the cabins for the survive?, or the age of the passanger influences in their supervivencies? find this, and make a script to use it is the job of the Data Sciences and sometimes help to do the world a better place and in other case, only to win one million dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
