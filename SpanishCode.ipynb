{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data y Machine Learning con Python\n",
    "\n",
    "## Parte practica\n",
    "\n",
    "### Prediciendo la supervivencia en el Titanic\n",
    "\n",
    "\n",
    "Para introducir los conceptos comentados en la presentacion teorica, vamos a participar en una competicion de Kaagle sobre la supervivencia de los pasajeros del Titanic.\n",
    "\n",
    "Esta competencion es una de las más populares para introducirse en el mundo de Machine Learning. La idea es conociendo lo que les ocurrio a una parte de los pasajeros del Titanic, predecir lo que le pasaría al resto.\n",
    "\n",
    "Para poder participar necesitariamos en primer lugar instalar python en nuestro sistema. También necesitamos instalar algunas bibliotecas para administrar datos.\n",
    "\n",
    "En concreto vamos a utilizar esas bibliotecas:\n",
    "\n",
    "    pandas: Nos permitira manejar diferentes formatos de datos (en nuestro caso trabajaremos con archivos.csv)\n",
    "    numpy: para trabajar con matrices, vectores y otros conjuntos de datos.\n",
    "    scikit-learn: Una biblioteca específica para Machine Learning que incluye diferentes estructuras de prediccion.\n",
    "    \n",
    "Utilizamos también otra biblioteca para ignorar los 'warnings' ya que python lanza un montón de advertencias sobre las diferentes versiones del lenguaje y nos distareria de nuestro verdadero objetivo.\n",
    "\n",
    "La forma más fácil de empezar a trabajar en Python es instalar Anaconda, un gestor de paquetes que incluye una gran cantidad de bibliotecas populares incluidas por supuesto, éstas.\n",
    "\n",
    "Anaconda instala ademas este Notebook que estamos utilizando, Jupyter, que como vereis es muy util para presentar codigo ya que permite ir ejecutandolo paso a paso e ir comentadolo.\n",
    "\n",
    "Para participar en una competicion de Kagle debemos ir a su sitio web descargar los documentos necesarios, que en este caso son dos:\n",
    "\n",
    "Https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "El primero es una lista de una parte de los pasajeros del Titanic e incluyen un campo que nos dice si sobrevivieron o no. El otro es la lista del resto de pasajeros pero sin este campo y es el que necesitamos para construir nuestras predicciones y enviar a Kaggle.\n",
    "\n",
    "Lo primero que haremos sera cargar nuestro documento y transformarlo en un DataFrame que es el tipo de objeto que maneja la libreria panda y con el que trabajaremos.\n",
    "\n",
    "Ahora podemos echar un vistazo a nuestros datos usando un método muy útil de la biblioteca panda: \n",
    "\n",
    "    head ()\n",
    "\n",
    "que nos muestran las cinco primeras filas de nuestro archivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train dataset\n",
    "train_url = \"dataset/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "# Take a look to the content\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este metodo podemos ver el nombre de las columnas que contiene nuestro \n",
    "\n",
    "\n",
    "\n",
    "Otro comando interesante para saber lo que contiene nuestro documento es \n",
    "\n",
    "    describe() \n",
    "\n",
    "que hace un resumen del contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los DataFrame contienen tambien un atributo\n",
    "\n",
    "    shape\n",
    "    \n",
    "que nos dira las dimensiones de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemnos visto, nuestro documento incluye una columna llamada ‘Survived’ a la que podemos acceder con la anotacion estandar de corchetes. y luego como el metodo .value_counts() podemos hacernos una idea de lo que ocurrio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"Survived\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se trabaja con datos suele ser mas normal que en lugar de trabajar con datos absolutos se trabaje con porcentajes, y por supuesto para esto tambien panda nos brinda un parametro que podemos pasar al metodo .value_counts(); normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of Survivor column\n",
    "print(train[\"Survived\"].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asi pues, con estos datos ya sabemos que lo mas habitual es que cualquiera que se subiera a bordo del Titanic acabara ahogado, asi que nuestra primera prediccion podria ser decir que todos los que suban moriran. Asi que adelante, creemos nuestra primera prediccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacerla cogeremos el archivo llamado test que es el que se nos proporciona para probar nuestos algoritmos, haremos una copia del mismo y realizaremos nuestra prediccion.\n",
    "\n",
    "Si queremos subir nuestra prediccion a Kaggle necesitamos cumplir los requisitos de participacion y que podemos ver en: https://www.kaggle.com/c/titanic#evaluation\n",
    "\n",
    "Ahi podemos leer:\n",
    "\n",
    "    Submission File Format\n",
    "    You should submit a csv file with exactly 418 entries plus a header row. \n",
    "    Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n",
    "    The file should have exactly 2 columns:\n",
    "    PassengerId (sorted in any order)\n",
    "    Survived (contains your binary predictions: 1 for survived, 0 for deceased) \n",
    "\n",
    "Asi que manos a la obra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Load the test dataset\n",
    "test_url = \"dataset/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "# Duplicate our dataset\n",
    "myTestTest = test.copy()\n",
    "\n",
    "# Set Survived column to 0\n",
    "myTestTest['Survived'] = 0\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myTestTest[\"PassengerId\"]).astype(int)\n",
    "Survived = np.array(myTestTest[\"Survived\"]).astype(int)\n",
    "testSolution = pd.DataFrame(Survived, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name testSolution.csv\n",
    "testSolution.columns = ['Survived']\n",
    "testSolution.to_csv(\"testSolution.csv\", index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si subimos esta solucion a Kaggle veremos que obtenemos una puntuacion del 62% lo que no esta mal para empezar, pero como hemos visto ha sido una prediccion manual, vamos a intentar mejorar nuestra puntuacion utilizando tecnicas de Machine Learning.\n",
    "\n",
    "En primer lugar vamos a utlizar un arbol de decisiones.\n",
    "\n",
    "* #### Arbol de decisiones (Decision Tree): \n",
    "Un arbol de decisiones es una estructura logica a la que se le proporciona cierta informacion y siguiendo unas reglas nos ofrece un resultado.\n",
    "\n",
    "\n",
    "Lo primero vamos a duplicar nuestros datos para no modificar los originales. Despues los normalizamos y harmonizamos. \n",
    "\n",
    "* #### Normalizamos y harmonizar datos:\n",
    "Estos dos conceptos son muy similares aunque incluyen un pequeño matiz que los diferencia: Normalizar se refiere al hecho de comprobar que nuestros datos se ajustan a los requerimientos que los datos deben tener, y harmonizar se refiere a ajustar los datos a los requerimientos necesarios para un trabajo especifico. Como se puede ver es la diferencia es minima y de hecho en muchos casos las palabras se usan indistintamente.\n",
    "\n",
    "Vamos a asignar valores numericos a nuestros datos que son mas faciles de tratar. Asignaremos un 1 a las mujeres y un 0 a los hombres. Tambien vamos a asignar 0 a la clase S, 1 a la clase C y 2 a la clase Q de la columna \"Embarked\". Tambien, para asegurarnos de que todos los pasajeros pertenecen a una clase, vamos a rellenar los posibles huecos con la clase mas comun. Veamos cual es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     889\n",
      "unique      3\n",
      "top         S\n",
      "freq      644\n",
      "Name: Embarked, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train[\"Embarked\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asi pues, rellenaremos los posibles huecos con la S. Para normalizar tambien la columna \"Age\", vamos a asignar a aquellos pasajeros que no tengan edad, la media de la edad de los que si la tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicate our dataset\n",
    "myTrain = train.copy()\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "myTrain[\"Sex\"][myTrain[\"Sex\"] == \"male\"] = 0\n",
    "myTrain[\"Sex\"][myTrain[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Fill the gaps in the Embarked variable\n",
    "myTrain[\"Embarked\"] = myTrain[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"S\"] = 0\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"C\"] = 1\n",
    "myTrain[\"Embarked\"][myTrain[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "# Fill the gaps in the Age variable\n",
    "myTrain[\"Age\"] = myTrain[\"Age\"].fillna(myTrain[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos harmonizado nuestros datos podemos empezar a crear nuestro primer arbol de decision. Vamos a usar otra popular libreria para hacerlo: sklearn. Con esta libreria podemos crear el arbol, y despues decirle lo que queremos conseguir con el (target), y con que elementos queremos que lo logre (features).\n",
    "\n",
    "Ahora podemos ver la importancia que nuestro algoritmo da a cada elemento con el atributo .feature_importances_, y comprobar que puntuacion obtendremos con el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12985597  0.31274009  0.22882793  0.328576  ]\n",
      "0.977553310887\n"
     ]
    }
   ],
   "source": [
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "# Create the target and features \n",
    "target = myTrain[\"Survived\"].values\n",
    "features_one = myTrain[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guauuuuu!!!!!! Obtendremos mas de un 97% de acierto. Con un algoritmo asi podemos hacernos ricos, teniendo en cuanta lo dificil que es realizar algoritmos que superen el 80% de acierto.\n",
    "\n",
    "Pero antes de subir nuestra prediccion a Kaggle echemos un ojo al arbol de decision que hemos creado\n",
    "\n",
    "Para hacerlo vamos a descargarnos el archivo .dot que es el formato usado por la libreria sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"decisionTree.dot\", 'w') as archivo_dot:\n",
    "  \t    tree.export_graphviz(my_tree_one, out_file = archivo_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego podemos convertir este archivo a png usando el programa del site de graphviz (herramientas open source para el manejo de esos archivos .dot)\n",
    "\n",
    "http://www.graphviz.org/Download_windows.php\n",
    "\n",
    "Para evitar perdidas de tiempo he realizado yo anteriormente todo este proceso y este es el grafico del arbol de decision que hemos utilizado:\n",
    "\n",
    "![caption](img/firstDecisionTree.png)\n",
    "\n",
    "Bueno, como podeis ver es bastante complicado, pero eso no nos importa, lo importante es su eficacia y ya hemos visto que alcanza casi un 98%.\n",
    "\n",
    "Bueno, usemos nuestro arbol de decision para hacer nuestra primera prediccion usando ya tecnicas de Machine Learning.\n",
    "\n",
    "Para ello tenemos que normalizar y harmonizar tambien los datos de test que vamos a usar, y ademas, si estudiamos los datos, veriamos que en este caso tambien esta vacia  la posicion 152 referente a la 'fare'(tarifa), asi que nuevamente le asignaremos la media del resto de datos que si poseemos. \n",
    "\n",
    "Despues, creamos nuestra pimera preddiccion con los datos del test, y creamos nuestro archivo .csv que subiremos a Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicate our dataset\n",
    "myFirstTest = test.copy()\n",
    "\n",
    "# Impute the missing value with the median\n",
    "myFirstTest.Fare[152] = myFirstTest[\"Fare\"].median()\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "myFirstTest[\"Sex\"][myFirstTest[\"Sex\"] == \"male\"] = 0\n",
    "myFirstTest[\"Sex\"][myFirstTest[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Fill the gaps in the Embarked variable\n",
    "myFirstTest[\"Embarked\"] = myFirstTest[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Fill the gaps in the Age variable\n",
    "myFirstTest[\"Age\"] = myFirstTest[\"Age\"].fillna(myFirstTest[\"Age\"].median())\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"S\"] = 0\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"C\"] = 1\n",
    "myFirstTest[\"Embarked\"][myFirstTest[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "myFirstTest_features = myFirstTest[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Make a prediction using the test set\n",
    "myFirstprediction = my_tree_one.predict(myFirstTest_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myFirstTest[\"PassengerId\"]).astype(int)\n",
    "firstSolution = pd.DataFrame(myFirstprediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name firstSolution.csv\n",
    "firstSolution.columns = ['Survived']\n",
    "firstSolution.to_csv(\"firstSolution.csv\", index_label = [\"PassengerId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si subimos nuestra solucion a Kaggle obtendremos una decepcionante puntuacion del 72%... que ha pasado?\n",
    "\n",
    "Es un buen momento para explicar otro interesante concepto del Machine Learning: el Sobreentrenamiento.\n",
    "    \n",
    "* #### Sobreentrenamiento (Overfitting):\n",
    "Cuanto entrenamos a nuestro modelo es posible que lo hagamos tan bien que solo se adapte a los datos que usamos para entrenarlo. La idea es que nuestro modelo pueda generalizarse con cualquier tipo de datos que usemos, no solo los que usamos de entrenamiento.\n",
    "\n",
    "En nuestro caso, hemos construido nuestro arbol de decision tan complicado que solo es capaz de amoldarse bien a los datos usados para entrenarlo, y de hecho, con esos datos, obtenia hasta un 97% de aciertos, pero al usar otro datos su rendimiento baja hasta el 72%<br>\n",
    "Para evitar el sobreentrenamiento podemos limitar la profundidad de nuestro arbol de decisiones y asi aumentar su capacidad de genaralizacion. Para hacer eso usamos dos argumentos a la hora de construir nuestro arbol: max_depth, que determina la profuncidad del mismo, y min_samples_split que indica las minimas muestra necesarias para seguir profundizando.<br>\n",
    "Uno de los decisiones que se deben tomar a la hora de crear un buen algoritmo es el uso de estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832772166105\n"
     ]
    }
   ],
   "source": [
    "# Fit your first decision tree: my_tree_one\n",
    "max_depth = 4\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_one, target)\n",
    "\n",
    "print(my_tree_two.score(features_one, target))\n",
    "\n",
    "# Make a prediction using the test set\n",
    "mySecondPrediction = my_tree_two.predict(myFirstTest_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(myFirstTest[\"PassengerId\"]).astype(int)\n",
    "secondSolution = pd.DataFrame(mySecondPrediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name secondSolution.csv\n",
    "secondSolution.columns = ['Survived']\n",
    "secondSolution.to_csv(\"secondSolution.csv\", index_label = [\"PassengerId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos el resultado de este algoritmo es sensiblemente menor, un 83%, pero por el contrario, si subimos el resultado de su uso en los datos de prueba alcanzamos un 76%, que es mayor que el anterior.<br><br>\n",
    "Ahora podemos tambien crear la imagen para este nuevo arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"secondSolution.dot\", 'w') as archivo_dot:\n",
    "  \t    tree.export_graphviz(my_tree_two, out_file = archivo_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, es mucho mas simple\n",
    "\n",
    "![caption](img/secondDecisionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, este arbol de decision consiguio mejorar nuestra puntuacion en Kaggle, pero aun podemos mejorarla usando otro interesante concepto de Machine Learning: los Bosques Aleatorios\n",
    "\n",
    "* #### Bosques Aleatorios (Random Forest):\n",
    "Un Bosque Aleatorio es basicamente un bosque de arboles de decisiones. Podemos fabricar gran cantidad de arboles de decisiones y evaluar el resultado de cada una de ellos, y asignar el resultado final a la decision mas comun de esos arboles.\n",
    "\n",
    "Vamos a crear un algoritmo usando esta tecnica.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841750841751\n"
     ]
    }
   ],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "myTrain_two = myTrain.copy()\n",
    "\n",
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creating features for train\n",
    "features_forest = myTrain_two[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 4, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "# Duplicate our test dataset\n",
    "mySecondTest = myFirstTest.copy()\n",
    "\n",
    "# Creating features for test\n",
    "test_features = mySecondTest[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Make a prediction using the test set\n",
    "myLastPrediction = my_forest.predict(test_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains our predictions\n",
    "PassengerId = np.array(mySecondTest[\"PassengerId\"]).astype(int)\n",
    "LastSolution = pd.DataFrame(myLastPrediction, PassengerId)\n",
    "\n",
    "# Write our solution to a csv file with the name secondSolution.csv\n",
    "LastSolution.columns = ['Survived']\n",
    "LastSolution.to_csv(\"LastSolution.csv\", index_label = [\"PassengerId\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Con este nuevo algoritmo conseguimos un resultado de cerca del 79%. Es un buen resultado. De todos modos seguro que aun podriamos mejorarlo usando nuevas formulas y encontrando nuevas variables. Por ejemplo, era importante la situacion de las cabinas de los supervivientes? Tiene alguna influencia la edad de los pasajeros? Encontrar esta correlaciones y crear algoritmos que los contemplen es el trabajo de los expertos en Machine Learning, que a veces les lleva a hacer del mundo un lugar mejor, y otras tan solo ha ganar un millon de dolares.<br><br>\n",
    "\n",
    "Muchas gracias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
